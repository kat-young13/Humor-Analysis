{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sem Eval Task - Detecting Humor in News Headlines\n",
    "By: Kat Young, Varsha, Humera\n",
    "\n",
    "\n",
    "### Data Preprocessing - written by Kat, Varsha, and Humera\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/kat/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/kat/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from string import digits\n",
    "import nltk\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "################ Read in Data Set ################\n",
    "dataset = pd.read_csv(\"train.csv\")\n",
    "\n",
    "\n",
    "######### Remove entries with 0 grades ##########\n",
    "dataset = dataset[dataset['grades'] != 0]\n",
    "\n",
    "\n",
    "##### Drop unnecessary feature cols \"id\" and \"grades\" #####\n",
    "dataset = dataset.drop(['id','grades'], axis=1)\n",
    "\n",
    "\n",
    "#### Replace word in news headline and save to new col ####\n",
    "dataset['replaced_sentence'] = \"\"\n",
    "storage_array = []\n",
    "for index, row in dataset.iterrows():\n",
    "    new = re.sub('<.*/>', row['edit'], row['original'], flags=re.DOTALL)\n",
    "    storage_array.append(new)\n",
    "dataset['replaced_sentence'] = storage_array\n",
    "\n",
    "\n",
    "############ Convert all characters to lowercase ############\n",
    "dataset['replaced_sentence'] = dataset['replaced_sentence'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "\n",
    "\n",
    "##################### Remove Stop Words #####################\n",
    "stop = stopwords.words('english')\n",
    "dataset['replaced_sentence'] = dataset['replaced_sentence'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "\n",
    "\n",
    "##################### Remove punctuation ##################### \n",
    "dataset['replaced_sentence'] = dataset['replaced_sentence'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "\n",
    "############ Remove common words along with 's' and 'nt' ############\n",
    "frequent_words = pd.Series(' '.join(dataset['replaced_sentence']).split()).value_counts()[:10]\n",
    "words_to_remove = ['s', 'nt']\n",
    "dataset['replaced_sentence'] = dataset['replaced_sentence'].apply(lambda x: \" \".join(x for x in x.split() if x not in words_to_remove))\n",
    "\n",
    "\n",
    "##################### Remove rare words ##################### \n",
    "rare = pd.Series(' '.join(dataset['replaced_sentence']).split()).value_counts()[-10:]\n",
    "rare = list(rare.index)\n",
    "dataset['replaced_sentence'] = dataset['replaced_sentence'].apply(lambda x: \" \".join(x for x in x.split() if x not in rare))\n",
    "\n",
    "\n",
    "####################### Remove Digits #######################\n",
    "dataset['replaced_sentence'] = dataset['replaced_sentence'].str.replace('\\d+', '')\n",
    "\n",
    "\n",
    "############ Create new column with tokenized words ############\n",
    "tokenized = [nltk.word_tokenize(sent) for sent in dataset['replaced_sentence']]\n",
    "dataset['tokenized'] = tokenized\n",
    "\n",
    "\n",
    "####################### Lemmatize Words #######################\n",
    "lemm = WordNetLemmatizer()\n",
    "lemm_each_row = []\n",
    "for index, row in dataset.iterrows():\n",
    "    lemmatized_output = ' '.join([lemm.lemmatize(w) for w in row['tokenized']])\n",
    "    lemmatized_output = lemmatized_output.split(' ')\n",
    "    lemm_each_row.append(lemmatized_output)\n",
    "dataset['tokenized'] = lemm_each_row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Load vectors directly from the file #############\n",
    "model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "vectors_array = []\n",
    "\n",
    "####### Get the word embeddings for each token in the model, store in column #######\n",
    "for index, row in dataset.iterrows():\n",
    "    each_sentence = []\n",
    "    for x in row['tokenized']:\n",
    "        if x in model:\n",
    "            vectors = model[x]\n",
    "            each_sentence.append(vectors)\n",
    "    vectors_array.append(each_sentence)\n",
    "\n",
    "dataset['word embeddings'] = vectors_array\n",
    "\n",
    "## Written by Kat, Varsha, and Humera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Sentence Embeddings - by Kat\n",
    "\n",
    "Sentence embeddings here are in the form of a one 300-dimension vector that represents the whole sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>edit</th>\n",
       "      <th>meanGrade</th>\n",
       "      <th>replaced_sentence</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>word embeddings</th>\n",
       "      <th>sentence embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>France is ‘ hunting down its citizens who join...</td>\n",
       "      <td>twins</td>\n",
       "      <td>0.2</td>\n",
       "      <td>france hunting citizens joined twins without t...</td>\n",
       "      <td>[france, hunting, citizen, joined, twin, witho...</td>\n",
       "      <td>[[-0.20605469, -0.16699219, 0.19238281, 0.2490...</td>\n",
       "      <td>[-0.01800537109375, 0.071014404296875, -0.0436...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pentagon claims 2,000 % increase in Russian tr...</td>\n",
       "      <td>bowling</td>\n",
       "      <td>1.6</td>\n",
       "      <td>pentagon claims  increase russian trolls bowli...</td>\n",
       "      <td>[pentagon, claim, increase, russian, troll, bo...</td>\n",
       "      <td>[[-0.15722656, 0.095214844, 0.203125, 0.242187...</td>\n",
       "      <td>[-0.081390380859375, -0.002872467041015625, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Iceland PM Calls Snap Vote as Pedophile Furor ...</td>\n",
       "      <td>party</td>\n",
       "      <td>1.0</td>\n",
       "      <td>iceland pm calls snap vote pedophile furor cra...</td>\n",
       "      <td>[iceland, pm, call, snap, vote, pedophile, fur...</td>\n",
       "      <td>[[0.06298828, -0.012268066, 0.060302734, 0.185...</td>\n",
       "      <td>[0.04075792100694445, -0.04292127821180555, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In an apparent first , Iran and Israel &lt;engage...</td>\n",
       "      <td>slap</td>\n",
       "      <td>0.4</td>\n",
       "      <td>apparent first iran israel slap militarily</td>\n",
       "      <td>[apparent, first, iran, israel, slap, militarily]</td>\n",
       "      <td>[[0.079589844, 0.10498047, -0.33398438, 0.1894...</td>\n",
       "      <td>[0.01416015625, -0.026163736979166668, 0.00514...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>All 22 &lt;promises/&gt; Trump made in his speech to...</td>\n",
       "      <td>sounds</td>\n",
       "      <td>1.2</td>\n",
       "      <td>sounds trump made speech congress one chart</td>\n",
       "      <td>[sound, trump, made, speech, congress, one, ch...</td>\n",
       "      <td>[[-0.02746582, -0.0065612793, -0.122558594, -0...</td>\n",
       "      <td>[0.004237583705357143, -0.013179234095982142, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            original     edit  meanGrade  \\\n",
       "0  France is ‘ hunting down its citizens who join...    twins        0.2   \n",
       "1  Pentagon claims 2,000 % increase in Russian tr...  bowling        1.6   \n",
       "2  Iceland PM Calls Snap Vote as Pedophile Furor ...    party        1.0   \n",
       "3  In an apparent first , Iran and Israel <engage...     slap        0.4   \n",
       "5  All 22 <promises/> Trump made in his speech to...   sounds        1.2   \n",
       "\n",
       "                                   replaced_sentence  \\\n",
       "0  france hunting citizens joined twins without t...   \n",
       "1  pentagon claims  increase russian trolls bowli...   \n",
       "2  iceland pm calls snap vote pedophile furor cra...   \n",
       "3         apparent first iran israel slap militarily   \n",
       "5        sounds trump made speech congress one chart   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  [france, hunting, citizen, joined, twin, witho...   \n",
       "1  [pentagon, claim, increase, russian, troll, bo...   \n",
       "2  [iceland, pm, call, snap, vote, pedophile, fur...   \n",
       "3  [apparent, first, iran, israel, slap, militarily]   \n",
       "5  [sound, trump, made, speech, congress, one, ch...   \n",
       "\n",
       "                                     word embeddings  \\\n",
       "0  [[-0.20605469, -0.16699219, 0.19238281, 0.2490...   \n",
       "1  [[-0.15722656, 0.095214844, 0.203125, 0.242187...   \n",
       "2  [[0.06298828, -0.012268066, 0.060302734, 0.185...   \n",
       "3  [[0.079589844, 0.10498047, -0.33398438, 0.1894...   \n",
       "5  [[-0.02746582, -0.0065612793, -0.122558594, -0...   \n",
       "\n",
       "                                 sentence embeddings  \n",
       "0  [-0.01800537109375, 0.071014404296875, -0.0436...  \n",
       "1  [-0.081390380859375, -0.002872467041015625, 0....  \n",
       "2  [0.04075792100694445, -0.04292127821180555, -0...  \n",
       "3  [0.01416015625, -0.026163736979166668, 0.00514...  \n",
       "5  [0.004237583705357143, -0.013179234095982142, ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as geek\n",
    "\n",
    "## This array holds all final sentence embeddings\n",
    "sentence_embeddings_array = []\n",
    "\n",
    "word_embedding_dim = len(dataset['word embeddings'][0][0])\n",
    "\n",
    "for index, row in dataset.iterrows():\n",
    "    base = np.zeros(word_embedding_dim)\n",
    "    count = 0\n",
    "    for x in row['word embeddings']:\n",
    "        base = geek.add(base, x)\n",
    "        count += 1\n",
    "    for i in range(0, len(base)):\n",
    "        base[i] = base[i] / count\n",
    "    sentence_embeddings_array.append(base)\n",
    "    \n",
    "dataset['sentence embeddings'] = sentence_embeddings_array\n",
    "\n",
    "dataset.head()\n",
    "\n",
    "## Written by Kat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into X_train and y_train - by Kat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9092    [0.107666015625, 0.0622711181640625, 0.1437683...\n",
       "2893    [-0.023319244384765625, 0.03894233703613281, 0...\n",
       "2787    [-0.013985373757102272, 0.08225319602272728, 0...\n",
       "8002    [0.08993094308035714, 0.10745675223214286, 0.1...\n",
       "5057    [-0.05480194091796875, -0.02362060546875, -0.0...\n",
       "Name: sentence embeddings, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset['sentence embeddings'],\n",
    "                                                    dataset['meanGrade'],\n",
    "                                                    test_size=1/4, random_state=0)\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model - by Kat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_len = 300\n",
    "\n",
    "X_train = pad_sequences(X_train,maxlen=max_len,padding='post' )\n",
    "X_test = pad_sequences(X_test, maxlen=max_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Embedding, Dense, LSTM, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "#input dimension is length of word embedding\n",
    "#output dimension is how many nodes on output layer\n",
    "model.add(Embedding(input_dim=300, output_dim=1, input_length=max_len))\n",
    "\n",
    "model.add(LSTM(units=300, dropout=0.3, recurrent_dropout=0.2))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 300, 1)            300       \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 300)               362400    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 301       \n",
      "=================================================================\n",
      "Total params: 363,001\n",
      "Trainable params: 363,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mse', metrics=['accuracy', 'mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 6846 samples, validate on 2283 samples\n",
      "Epoch 1/1\n",
      "6846/6846 [==============================] - 135s 20ms/step - loss: 0.3110 - acc: 0.1266 - mean_squared_error: 0.3110 - val_loss: 0.3104 - val_acc: 0.1248 - val_mean_squared_error: 0.3104\n"
     ]
    }
   ],
   "source": [
    "result= model.fit(X_train, y_train, batch_size=32, \n",
    "                  epochs=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM RMSE base: 0.5576421632088139\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "LSTM_RMSE = math.sqrt(result.history['mean_squared_error'][0])\n",
    "print(\"LSTM RMSE base: {}\".format(LSTM_RMSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional LSTM - by Kat and Humera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Embedding, Dense, LSTM, Dropout, Bidirectional\n",
    "\n",
    "biLSTM_model = Sequential()\n",
    "\n",
    "biLSTM_model.add(Embedding(input_dim=300, output_dim=1, input_length=max_len))\n",
    "\n",
    "biLSTM_model.add(Bidirectional(LSTM(units=300, dropout=0.3, recurrent_dropout=0.2)))\n",
    "\n",
    "biLSTM_model.add(Dense(1, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 300, 1)            300       \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 600)               724800    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 601       \n",
      "=================================================================\n",
      "Total params: 725,701\n",
      "Trainable params: 725,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "biLSTM_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6846 samples, validate on 2283 samples\n",
      "Epoch 1/1\n",
      "6846/6846 [==============================] - 229s 33ms/step - loss: 0.3108 - acc: 0.1266 - mean_squared_error: 0.3108 - val_loss: 0.3104 - val_acc: 0.1248 - val_mean_squared_error: 0.3104\n"
     ]
    }
   ],
   "source": [
    "biLSTM_model.compile(optimizer='adam', loss='mse', metrics=['accuracy', 'mse'])\n",
    "\n",
    "biLSTM_result = biLSTM_model.fit(X_train, y_train, batch_size=32, \n",
    "                  epochs=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biLSTM RMSE base: 0.5575103067837445\n"
     ]
    }
   ],
   "source": [
    "biLSTM_RMSE = math.sqrt(biLSTM_result.history['mean_squared_error'][0])\n",
    "print(\"biLSTM RMSE base: {}\".format(biLSTM_RMSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## biLSTM - more layers - by Kat and Humera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Embedding, Dense, LSTM, Dropout, Bidirectional\n",
    "\n",
    "biLSTM_model_b = Sequential()\n",
    "biLSTM_model_b.add(Embedding(input_dim=300, output_dim=1, input_length=max_len))\n",
    "#input dimension is length of word embedding\n",
    "#output dimension is how many nodes on output layer\n",
    "biLSTM_model_b.add(Bidirectional(LSTM(units=300, dropout=0.3, recurrent_dropout=0.2)))\n",
    "\n",
    "biLSTM_model_b.add(Dense(64, activation='sigmoid'))\n",
    "\n",
    "biLSTM_model_b.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 300, 1)            300       \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 600)               724800    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                38464     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 763,629\n",
      "Trainable params: 763,629\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "biLSTM_model_b.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6846 samples, validate on 2283 samples\n",
      "Epoch 1/1\n",
      "6846/6846 [==============================] - 232s 34ms/step - loss: 0.3166 - acc: 0.1233 - mean_squared_error: 0.3166 - val_loss: 0.3103 - val_acc: 0.1248 - val_mean_squared_error: 0.3103\n"
     ]
    }
   ],
   "source": [
    "biLSTM_model_b.compile(optimizer='adam', loss='mse', metrics=['accuracy', 'mse'])\n",
    "\n",
    "biLSTM_result_b = biLSTM_model_b.fit(X_train, y_train, batch_size=32, \n",
    "                  epochs=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biLSTM RMSE (additional layer): 0.5626690928067789\n"
     ]
    }
   ],
   "source": [
    "biLSTM_RMSE_b = math.sqrt(biLSTM_result_b.history['mean_squared_error'][0])\n",
    "print(\"biLSTM RMSE (additional layer): {}\".format(biLSTM_RMSE_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## biLSTM - RELU - by Kat, Varsha, Humera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Embedding, Dense, LSTM, Dropout, Bidirectional\n",
    "\n",
    "biLSTM_model_c = Sequential()\n",
    "\n",
    "biLSTM_model_c.add(Embedding(input_dim=300, output_dim=1, input_length=max_len))\n",
    "\n",
    "biLSTM_model_c.add(Bidirectional(LSTM(units=300, dropout=0.3, recurrent_dropout=0.2)))\n",
    "\n",
    "biLSTM_model_c.add(Dense(1, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 300, 1)            300       \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 600)               724800    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 601       \n",
      "=================================================================\n",
      "Total params: 725,701\n",
      "Trainable params: 725,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "biLSTM_model_c.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6846 samples, validate on 2283 samples\n",
      "Epoch 1/1\n",
      "6846/6846 [==============================] - 237s 35ms/step - loss: 0.3385 - acc: 0.1239 - mean_squared_error: 0.3385 - val_loss: 0.3309 - val_acc: 0.1248 - val_mean_squared_error: 0.3309\n"
     ]
    }
   ],
   "source": [
    "biLSTM_model_c.compile(optimizer='adam', loss='mse', metrics=['accuracy', 'mse'])\n",
    "\n",
    "biLSTM_result_c = biLSTM_model_c.fit(X_train, y_train, batch_size=32, \n",
    "                  epochs=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biLSTM RMSE (relu): 0.5817875898138096\n"
     ]
    }
   ],
   "source": [
    "biLSTM_RMSE_c = math.sqrt(biLSTM_result_c.history['mean_squared_error'][0])\n",
    "print(\"biLSTM RMSE (relu): {}\".format(biLSTM_RMSE_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression - by Kat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Linear Regression to the dataset \n",
    "from sklearn.linear_model import LinearRegression \n",
    "lin = LinearRegression() \n",
    "  \n",
    "lin.fit(X_train, y_train) \n",
    "Y_pred = lin.predict(X_test)  # make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "lin_reg_MSE = mean_squared_error(y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression RMSE: 0.5571047393476176\n"
     ]
    }
   ],
   "source": [
    "lin_reg_RMSE = math.sqrt(lin_reg_MSE)\n",
    "print(\"Linear Regression RMSE: {}\".format(lin_reg_RMSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of Methods - by Kat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression RMSE: 0.5571047393476176\n",
      "biLSTM RMSE (relu): 0.5817875898138096\n",
      "biLSTM RMSE (additional layer): 0.5626690928067789\n",
      "biLSTM RMSE base: 0.5575103067837445\n",
      "LSTM RMSE base: 0.5576421632088139\n"
     ]
    }
   ],
   "source": [
    "print(\"Linear Regression RMSE: {}\".format(lin_reg_RMSE))\n",
    "print(\"biLSTM RMSE (relu): {}\".format(biLSTM_RMSE_c))\n",
    "print(\"biLSTM RMSE (additional layer): {}\".format(biLSTM_RMSE_b))\n",
    "print(\"biLSTM RMSE base: {}\".format(biLSTM_RMSE))\n",
    "print(\"LSTM RMSE base: {}\".format(LSTM_RMSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chosen Model - biLSTM - by Kat\n",
    "\n",
    "We chose biLSTM to be our best model since its results are second best (by a small margin) to Linear Regression RMSE. While the results for the Linear Regression are slightly better, we believe there is more potential for hyperparameter tuning (past what we have done already), which is why we explore the biLSTM model more. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run on 5 epochs to improve results with some improved hyperparamters we found. Running on one epoch doesn't allow the model to reach toward its potential, but was what we viewed as most efficient for comparing initial results. The main hyper parameters we changed from our base biLSTM model were batch size (from 32 to 16) and epochs (for the reason mentioned above). Other hyperparameters were tested out on various machines (which is why they aren't run in this jupyter notebook), such as changing the optimizer (i.e. from adam optimizer to SGD), having a higher batch size (which gave higher RMSE), changing activation function (sigmoid vs tanh vs relu), and adding another layer. For the most part, our base model performed better than new models with the edits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 300, 1)            300       \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 600)               724800    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 601       \n",
      "=================================================================\n",
      "Total params: 725,701\n",
      "Trainable params: 725,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Embedding, Dense, LSTM, Dropout, Bidirectional\n",
    "\n",
    "biLSTM_model_final = Sequential()\n",
    "\n",
    "biLSTM_model_final.add(Embedding(input_dim=300, output_dim=1, input_length=max_len))\n",
    "\n",
    "biLSTM_model_final.add(Bidirectional(LSTM(units=300, dropout=0.3, recurrent_dropout=0.2)))\n",
    "\n",
    "biLSTM_model_final.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "biLSTM_model_final.compile(optimizer='adam', loss='mse', metrics=['accuracy', 'mse'])\n",
    "\n",
    "biLSTM_model_final.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6846 samples, validate on 2283 samples\n",
      "Epoch 1/5\n",
      "6846/6846 [==============================] - 319s 47ms/step - loss: 0.3101 - acc: 0.1266 - mean_squared_error: 0.3101 - val_loss: 0.3104 - val_acc: 0.1248 - val_mean_squared_error: 0.3104\n",
      "Epoch 2/5\n",
      "6846/6846 [==============================] - 326s 48ms/step - loss: 0.3062 - acc: 0.1271 - mean_squared_error: 0.3062 - val_loss: 0.3104 - val_acc: 0.1248 - val_mean_squared_error: 0.3104\n",
      "Epoch 3/5\n",
      "6846/6846 [==============================] - 320s 47ms/step - loss: 0.3062 - acc: 0.1271 - mean_squared_error: 0.3062 - val_loss: 0.3104 - val_acc: 0.1248 - val_mean_squared_error: 0.3104\n",
      "Epoch 4/5\n",
      "6846/6846 [==============================] - 320s 47ms/step - loss: 0.3062 - acc: 0.1271 - mean_squared_error: 0.3062 - val_loss: 0.3104 - val_acc: 0.1248 - val_mean_squared_error: 0.3104\n",
      "Epoch 5/5\n",
      "6846/6846 [==============================] - 327s 48ms/step - loss: 0.3062 - acc: 0.1271 - mean_squared_error: 0.3062 - val_loss: 0.3104 - val_acc: 0.1248 - val_mean_squared_error: 0.3104\n"
     ]
    }
   ],
   "source": [
    "biLSTM_result_final = biLSTM_model_final.fit(X_train, y_train, batch_size=16, \n",
    "                  epochs=5, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the scores from each epoch. Running each additional epoch does improve results, but only by an extremely small margin, which makes the additional resources and time past the second epoch not very beneficial while being very costly. Two epochs appears to be a good number for our purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3100949605149273, 0.30615509604192287, 0.3061550952888113, 0.3061550942353258, 0.3061550947664218]\n"
     ]
    }
   ],
   "source": [
    "print(biLSTM_result_final.history['mean_squared_error'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Validation - biLSTM (2 epochs) - by Kat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'X': dataset[\"sentence embeddings\"], 'Y': dataset[\"meanGrade\"]}\n",
    "\n",
    "df_test = pd.DataFrame(data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## split data into 10 parts\n",
    "## run for loop for each part\n",
    "\n",
    "def biLSTM_ten_fold_cross_val(data):\n",
    "    val_scores = []                            # array for validation scores\n",
    "    \n",
    "    shuffled = data.sample(frac=1)             # shuffle the data (to keep randomness)\n",
    "    the_split = np.array_split(data, 10)       # split into ten parts. access each part through the_split[x]\n",
    "    \n",
    "    for i in range(0,10):\n",
    "        print(\"biLSTM Validation {} of 10\".format(i + 1))\n",
    "        validation_set = the_split[i]\n",
    "        train_frames = []\n",
    "        for p in range(0,10):\n",
    "            if p != i:\n",
    "                train_frames.append(the_split[p])\n",
    "\n",
    "        train_set = pd.concat(train_frames)    # this is the training set (with x and y) for this round.\n",
    "        \n",
    "        X_train = train_set[\"X\"]\n",
    "        Y_train = train_set[\"Y\"]\n",
    "        X_test = validation_set[\"X\"]\n",
    "        Y_test = validation_set[\"Y\"]\n",
    "        \n",
    "        X_train = pad_sequences(X_train,maxlen=max_len,padding='post' )\n",
    "        X_test = pad_sequences(X_test, maxlen=max_len, padding='post')\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Embedding(input_dim=300, output_dim=1, input_length=max_len))\n",
    "        model.add(Bidirectional(LSTM(units=300, dropout=0.3, recurrent_dropout=0.2)))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        model.compile(optimizer='adam', loss='mse', metrics=['accuracy', 'mse'])\n",
    "\n",
    "        result = model.fit(X_train, Y_train, batch_size=16, \n",
    "                      epochs=2, validation_data=(X_test, Y_test))\n",
    "        \n",
    "        val_scores.append(result.history['mean_squared_error'])\n",
    "        \n",
    "    return val_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biLSTM Validation 1 of 10\n",
      "Train on 8216 samples, validate on 913 samples\n",
      "Epoch 1/2\n",
      "8216/8216 [==============================] - 368s 45ms/step - loss: 0.3107 - acc: 0.1262 - mean_squared_error: 0.3107 - val_loss: 0.2922 - val_acc: 0.1292 - val_mean_squared_error: 0.2922\n",
      "Epoch 2/2\n",
      "8216/8216 [==============================] - 370s 45ms/step - loss: 0.3089 - acc: 0.1262 - mean_squared_error: 0.3089 - val_loss: 0.2922 - val_acc: 0.1292 - val_mean_squared_error: 0.2922\n",
      "biLSTM Validation 2 of 10\n",
      "Train on 8216 samples, validate on 913 samples\n",
      "Epoch 1/2\n",
      "8216/8216 [==============================] - 366s 45ms/step - loss: 0.3078 - acc: 0.1271 - mean_squared_error: 0.3078 - val_loss: 0.3179 - val_acc: 0.1216 - val_mean_squared_error: 0.3179\n",
      "Epoch 2/2\n",
      "8216/8216 [==============================] - 362s 44ms/step - loss: 0.3060 - acc: 0.1271 - mean_squared_error: 0.3060 - val_loss: 0.3179 - val_acc: 0.1216 - val_mean_squared_error: 0.3179\n",
      "biLSTM Validation 3 of 10\n",
      "Train on 8216 samples, validate on 913 samples\n",
      "Epoch 1/2\n",
      "8216/8216 [==============================] - 364s 44ms/step - loss: 0.3115 - acc: 0.1262 - mean_squared_error: 0.3115 - val_loss: 0.3028 - val_acc: 0.1260 - val_mean_squared_error: 0.3028\n",
      "Epoch 2/2\n",
      "8216/8216 [==============================] - 4942s 602ms/step - loss: 0.3077 - acc: 0.1266 - mean_squared_error: 0.3077 - val_loss: 0.3028 - val_acc: 0.1260 - val_mean_squared_error: 0.3028\n",
      "biLSTM Validation 4 of 10\n",
      "Train on 8216 samples, validate on 913 samples\n",
      "Epoch 1/2\n",
      "8216/8216 [==============================] - 2384s 290ms/step - loss: 0.3102 - acc: 0.1268 - mean_squared_error: 0.3102 - val_loss: 0.2994 - val_acc: 0.1238 - val_mean_squared_error: 0.2994\n",
      "Epoch 2/2\n",
      "8216/8216 [==============================] - 362s 44ms/step - loss: 0.3081 - acc: 0.1268 - mean_squared_error: 0.3081 - val_loss: 0.2994 - val_acc: 0.1238 - val_mean_squared_error: 0.2994\n",
      "biLSTM Validation 5 of 10\n",
      "Train on 8216 samples, validate on 913 samples\n",
      "Epoch 1/2\n",
      "8216/8216 [==============================] - 378s 46ms/step - loss: 0.3106 - acc: 0.1267 - mean_squared_error: 0.3106 - val_loss: 0.3052 - val_acc: 0.1249 - val_mean_squared_error: 0.3052\n",
      "Epoch 2/2\n",
      "8216/8216 [==============================] - 373s 45ms/step - loss: 0.3074 - acc: 0.1267 - mean_squared_error: 0.3074 - val_loss: 0.3052 - val_acc: 0.1249 - val_mean_squared_error: 0.3052\n",
      "biLSTM Validation 6 of 10\n",
      "Train on 8216 samples, validate on 913 samples\n",
      "Epoch 1/2\n",
      "8216/8216 [==============================] - 368s 45ms/step - loss: 0.3108 - acc: 0.1260 - mean_squared_error: 0.3108 - val_loss: 0.2987 - val_acc: 0.1314 - val_mean_squared_error: 0.2987\n",
      "Epoch 2/2\n",
      "8216/8216 [==============================] - 368s 45ms/step - loss: 0.3082 - acc: 0.1260 - mean_squared_error: 0.3082 - val_loss: 0.2987 - val_acc: 0.1314 - val_mean_squared_error: 0.2987\n",
      "biLSTM Validation 7 of 10\n",
      "Train on 8216 samples, validate on 913 samples\n",
      "Epoch 1/2\n",
      "8216/8216 [==============================] - 374s 45ms/step - loss: 0.3108 - acc: 0.1259 - mean_squared_error: 0.3108 - val_loss: 0.3049 - val_acc: 0.1314 - val_mean_squared_error: 0.3049\n",
      "Epoch 2/2\n",
      "8216/8216 [==============================] - 357s 43ms/step - loss: 0.3075 - acc: 0.1260 - mean_squared_error: 0.3075 - val_loss: 0.3049 - val_acc: 0.1314 - val_mean_squared_error: 0.3049\n",
      "biLSTM Validation 8 of 10\n",
      "Train on 8216 samples, validate on 913 samples\n",
      "Epoch 1/2\n",
      "8216/8216 [==============================] - 370s 45ms/step - loss: 0.3086 - acc: 0.1262 - mean_squared_error: 0.3086 - val_loss: 0.3203 - val_acc: 0.1271 - val_mean_squared_error: 0.3203\n",
      "Epoch 2/2\n",
      "8216/8216 [==============================] - 377s 46ms/step - loss: 0.3058 - acc: 0.1265 - mean_squared_error: 0.3058 - val_loss: 0.3203 - val_acc: 0.1271 - val_mean_squared_error: 0.3203\n",
      "biLSTM Validation 9 of 10\n",
      "Train on 8216 samples, validate on 913 samples\n",
      "Epoch 1/2\n",
      "8216/8216 [==============================] - 369s 45ms/step - loss: 0.3078 - acc: 0.1274 - mean_squared_error: 0.3078 - val_loss: 0.3229 - val_acc: 0.1183 - val_mean_squared_error: 0.3229\n",
      "Epoch 2/2\n",
      "8216/8216 [==============================] - 360s 44ms/step - loss: 0.3055 - acc: 0.1274 - mean_squared_error: 0.3055 - val_loss: 0.3229 - val_acc: 0.1183 - val_mean_squared_error: 0.3229\n",
      "biLSTM Validation 10 of 10\n",
      "Train on 8217 samples, validate on 912 samples\n",
      "Epoch 1/2\n",
      "8217/8217 [==============================] - 5922s 721ms/step - loss: 0.3096 - acc: 0.1257 - mean_squared_error: 0.3096 - val_loss: 0.3078 - val_acc: 0.1316 - val_mean_squared_error: 0.3078\n",
      "Epoch 2/2\n",
      "8217/8217 [==============================] - 2886s 351ms/step - loss: 0.3073 - acc: 0.1260 - mean_squared_error: 0.3073 - val_loss: 0.3078 - val_acc: 0.1316 - val_mean_squared_error: 0.3078\n"
     ]
    }
   ],
   "source": [
    "biLSTM_cross_val = biLSTM_ten_fold_cross_val(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation 1: 0.5573987624169929, 0.5557729861873885\n",
      "Validation 2: 0.5547900375522811, 0.5531956939460426\n",
      "Validation 3: 0.5581081808398746, 0.5547156774391875\n",
      "Validation 4: 0.5569560534305721, 0.5550539641347731\n",
      "Validation 5: 0.5573316405305709, 0.5544757990175536\n",
      "Validation 6: 0.5575315204725305, 0.5551254392946691\n",
      "Validation 7: 0.5574718322690859, 0.5544982908250331\n",
      "Validation 8: 0.5555450724982951, 0.5529569491632386\n",
      "Validation 9: 0.5547703419501918, 0.5526916471404962\n",
      "Validation 10: 0.5564491083948483, 0.5543730390260023\n"
     ]
    }
   ],
   "source": [
    "def printRMSE(mse):\n",
    "    for i in range(0, len(mse)):\n",
    "        print(\"Validation {}: {}, {}\".format(i + 1, math.sqrt(mse[i][0]), math.sqrt(mse[i][1])))\n",
    "            \n",
    "            \n",
    "printRMSE(biLSTM_cross_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
