# SemEval-Task-7
Code for NLP Project on SemEval Task 7 (2020)

Member Roles: Each team member played an equal role in solving the problem, as we met in person to write code together and divided other work up equally. Varsha presented the first two sections of our presentation, Kat presented the second two, and Humera presented the last two. Varsha and Humera wrote the install file, and Kat wrote this README.md file.

Problem Explanation: The goal of the work we are doing is to write a model to detect humor in the way news headlines were edited. The original news headlines in the dataset are considered "not funny", but by replacing one word a headline has the potential to become funny. For example, the sentence 'Trump is caught promising to 'release the <memo/>' on hot mic, would change the word 'memo' to 'kraken' to make it potentially funny. We are coming up with a model to judge the headline funniness based on the training data in the dataset which was annotated by a group of real people for funniness ratings. Funniness is up to bias and discretion which makes humor a challenging concept to judge via an algorithm. 

Solution: To solve this problem, we began by processing the data from the dataset. First we obtained the sentence with the edit in place, that sentence divided into individual words of importance, word embeddings for each word in that sentence, as well as sentence embeddings. In the process of getting these new representations, we converted all words to lowercase, removed stop words, removed punctuation, removed some of the common words (such as 's' and 'nt', which represented posession and 'not'), removed rare words, and removed digits. The words were represented numerically through word embeddings. We got the word embeddings from GoogleNews-vectors. Then, we decided to look at running two different models: a neural network and a linear regression. The inputs to the neural network were the sentence embeddings. It ended up yielding a 12.41% accuracy. The inputs to the linear regression were the sentence embeddings, which were matched up with the funniness scores. A set of predictions included [0.99423773, 0.97713483, 1.0045303, 0.97696265, 0.99313956], while the actual scores were [0.2, 1.6, 1.0, 0.4, 1.2]. In this set of predictions the only predictions that were somewhat close to correct were the third and the last. To see how far off our regression model was, we looked at what is called the "root mean square error", which essentially told us generally how far away from the true value our predictions were. We obtained 10 different RMSE values by dividing the trianing set up into 10 sections and using each section as the testing section (a method called 10-fold cross-validation). All of the RMSE values were around 0.5, which means the values we got were all around 0.5 off from the true value. 

To run the program: Install all dependencies in the "install" file. Launch jupyter notebook. Click "Run All". It may take a little time because of the large volume of word embeddings from GoogleNews.
